---
title: "Digging into dada2"
author: "Emily Bean"
date: Sys.date()
output: 
  rmarkdown::github_document
---

### Overview

The dada2 algorithm was published [in Nature in 2016](https://www.nature.com/articles/nmeth.3869). It is available as a Bioconductor R package. 

Dada2 is able to run locally (does not require supercomputing abilities) and through RStudio, making it easily accessible to biologists who are not familiar with command line programming. It is intended for 16S or ITS Illumina-sequenced amplicon data but also supports pyrosequencing with [some additional parameter changes](https://benjjneb.github.io/dada2/faq.html#can-i-use-dada2-with-my-454-or-ion-torrent-data) as well as PacBio data.

This document will give a brief overview of the main dada2 pipeline, along with an explanation of each step in the process. 

From the Callahan et al. 2016 paper: 
>*"The DADA2 R package implements the full amplicon workflow: filtering, dereplication,  sample inference, chimera identification, and merging of paired-end reads."*

For this tutorial, we have downloaded all reads from 10 250x250 bp paired-end fastq files (20 files total) from NCBI accession number SRP135886 (code downloaded from: https://www.bioinformatics.recipes/recipe/view/recipe-230-095/#code). 
It is important to note that Windows does not support multithreading in this operation, so if working from a Windows machine you may want to further subset the sample data to run faster. 

### Installation

The dada2 package needs to be installed through Bioconductor. *Note: the latest version requires R 3.6*

```{#r}
## Skip this chunk if dada2 is already installed

# Install bioconductor

install.packages("BiocManager")
require(BiocManager)

# Install dada2 through Bioconductor
BiocManager::install("dada2", version = "3.10") # note that this needs R 3.6
```

```{#r}
# Once installed (or if already installed), load the package through R
require(dada2)
```

### Filter and Trim

Dada2 supports filtering and trimming fastq files within R, but this is an optional step. 
Important note: downstream `dada2` functions do not accept **any** ambiguous bases; these must all be removed during trimming/filtering.

The code chunk below reads fastq files from an absolute path. There is some error handling including to ensure that the files are read in correctly and sorted in order; `dada2` assumes that the forward and reverse files are ordered. 

```{r}
## ---- setVariables ----

# path to directory that contains fastq files
PATH = "~/dada2demo/"

# desired SRA numbers
SRA = list.files(PATH, pattern = ".fastq")

# paired end characterization; sequences downloaded from NCBI have patterns: "_1.fastq" for forward and "_2.fastq" for reverse
PATTERNF = "_1.fastq"
PATTERNR = "_2.fastq"

# Quality cut: "Truncate reads at the first instance of a quality score less than or equal to truncQ"
TRUNCQ = 0

# Length cut: "Truncate reads after `trunclen` bases; reads shorter than this are discarded"
# NOTE: if paired-end, need two values (one for forward and one for reverse)
TRUNCLEN = c(0, 0)

# Head trim: "Number of nucleotides to remove from the start of each read"
TRIMLEFT = 0

# Tail trim: "Number of nucleotides to remove from the end of each read"
TRIMRIGHT = 0

# Minimum length: "Remove reads with length less than minLen" -- this happens AFTER other trims/truncations
MINLEN = 100

# Maxmimum N: "After truncation, sequences with more than `maxN` are discarded"
# NOTE: dada2 does not allow Ns
MAXN = 0

# Minimum quality: "After truncation, reads contain a quality score less than `minQ` are discarded"
MINQ = 0

## ---- getFiles----

# are files already split? 
if(any(grepl(PATTERNF, SRA))) {
  
  # find forward and reverse files
  fwd <- SRA[grep(PATTERNF, SRA)]
  rev <- SRA[grep(PATTERNR, SRA)]
  
  # get file paths
  fwdFiles <- paste0(PATH, fwd)
  revFiles <- paste0(PATH, rev)
} else {  # if files are not split, get path
  
  # get forward files
  fwdFiles <- paste0(PATH, SRA, PATTERNF)
  # get reverse files
  revFiles <- paste0(PATH, SRA, PATTERNR)
  
}

# error catch
if(length(file(fwdFiles[1])) == 0) {
  
  stop("cannot read forward file path")
  
} 
# error catch
if(length(file(revFiles[1])) == 0) {
  
  stop("cannot read reverse file path")
}

# check to make sure that the lengths of both files are the same
if(length(fwdFiles) != length(revFiles)) {
  
  stop("There is an unequal number of forward and reverse files")
}

# get sample names
fwdNames <- sapply(strsplit(basename(fwdFiles), PATTERNF), `[`, 1)
revNames <- sapply(strsplit(basename(revFiles), PATTERNR), `[`, 1)

# error catch if unordered
if(any(!fwdNames %in% revNames)) {
  
  stop("forward and reverse files are out of order")
  
}
```

In the next code chunk, we perform the filtering and trimming step. This creates a subdirectory filled with the filtered files, and creates a dataframe that counts the reads that were removed during trimming. 

When paired reads are input, the function performs filtering on the forward and reverse reads independently, and only passes the reads that pass both. The `dada2` package has several functions that can filter and trim reads (see `?fastqFilter` and `fastqPairedFilter`), but the `filterAndTrim` function supports multithreading (on MacOS only).

If the `filterAndTrim` function is saved to a variable, the output is a matrix of input reads and output reads. This allows us to track the reads that we lose along the pipeline. For this demonstration, our filter and trimming parameters were very loose and we lost on average about 2% of the reads during this step.

```{r}
### ---- filterAndTrim ----

# create subdirectory for filtered files
filtForward <- file.path("./filtered", paste0(fwdNames, "_F_filt.fastq.gz"))
filtReverse <- file.path("./filtered", paste0(revNames, "_R_filt.fastq.gz"))


# filter and trim
cleaned <- filterAndTrim(fwd = fwdFiles, rev = revFiles,
                         filt = filtForward, filt.rev = filtReverse,
                         # add parameters that the user selected in previous chunk
                         truncQ = TRUNCQ,
                         truncLen = TRUNCLEN,
                         trimLeft = TRIMLEFT,
                         trimRight = TRIMRIGHT,
                         maxN = MAXN,
                         minLen = MINLEN,
                         minQ = MINQ
)

# track how many reads were removed during filtering process
head(cleaned)

# add a column to the tracked reads dataframe to see percentage of reads lost
track.reads <- as.data.frame(cleaned)
track.reads$percent.lost <- round((track.reads$reads.in - track.reads$reads.out) / track.reads$reads.in * 100, 2)

# view the dataframe
head(track.reads)

```


### dada2 Core Algorithms

The next two chunks assume that the sequencing data is cleaned, trimmed, and filtered to your standards (which should be high!). 

#### Perform core dada2 algorithms

There are multiple steps to this process, allowing for some user input at each step. There are two main data outputs: `seqtab.nochim`, the ASV table, and `taxa`, the table of assigned taxonomy for each ASV. The final (optional) step concatenates these neatly to a phyloseq object that the user can add sample metadata to and complete further analysis on.

`err`: Estimates the error rates by "alternating between sample inference and error rate estimation until convergence". Discovers model parameters from the data with unsupervised learning in which "the sample inference is alternated with parameter estimation until both are jointly consistent" - use with a subset of data

`dada`: "Removes all sequencing errors to reveal the members of the sequenced community". This is a de-noising step that removes Illumina errors. Basically, `dada` uses the `err` object to find Illumina or PCR sequencing errors, removes those, and then classifies the rest of the community similar to OTU clustering but by biological similarity instead. The output is a sequence x sample table similar to a traditional OTU table. `dada` requires multiple reads in each sample, since the core algorithm makes the assumption that biological sequences are more likely to be repeatedly observed than error sequences. It assumes that once the error sequences are removed, what is left is the actual DNA sequence of the organism. *Note that this makes dada2 less sensitive to rare variants, since they are more likely to be considered an error sequence*. The function creates a statistical test to test whether a sequence has been seen too many times to have been created by sequencing errors. It is a "parametric error model of substitutions", which means the accuracy of the error model (created by `err`) affects the sample inference.
* All comparisons depend on pairwise alignments; kmer-distance screen and banded Needleman-Wunsch alignment
* This is similar to what used to be used for 454 pyrosequencing, and is called a "denoising" process.
* The object returned by `dada` is a `dada-class` containing diagnostic information about the quality of each sample's denoising. See `?help(dada-class)`
* The basic tutorial workflow does *not* perform a dereplicating step and instead the filtered fastq files are input to the `dada` step.  


`mergePairs`: merges each denoised pair of forward and reverse reads
* Reads are rejected if there is not enough overlap or too many mismatches - these parameters can be manipulated to make merging more or less stringent.

`makeSequenceTable`: constructs an ASV table (similar to an OTU table) from the list of samples.

`removeBimeraDenovo`: removes chimeras from two identification methods: identifying across both pooled sequences and consensus across samples. 

`assignTaxonomy`: Uses naive Bayesian classifer to assign taxonomy based on a reference training set fasta of classifed sequences.

`addSpecies`: "...assign genus-species binomials to input sequences by exact matching against a reference fasta". These are merged into the taxonomic tables as an additional column, and species identification that match the input table and binomial classification are included in the return table.


```{#r}
# make sure the file path points to the listed files
if(length(list.files(CLEANEDPATH)) == 0) {
  stop("Path does not point to fastq files")
}

# perform dada2 algorithms
if(PAIRED == TRUE) {
  
  # get forward and reverse reads
  forward <- sort(list.files(CLEANEDPATH, pattern = PATTERNF, full.names = TRUE))
  reverse <- sort(list.files(CLEANEDPATH, pattern = PATTERNR, full.names = TRUE))
  
  # check to make sure that the lengths of both files are the same
  if(length(forward) != length(reverse)) {
    stop("Forward and reverse paths do not match; are the samples uneven?")
  }
  
  # perform error learning
  errF <- learnErrors(forward, multithread = TRUE)
  errR <- learnErrors(reverse, multithread = TRUE)
  
  # perform dada2
  dadaForward <- dada(forward, err = errF, multithread = TRUE)
  dadaReverse <- dada(reverse, err = errR, multithread = TRUE)
  
  # merge paired reads
  mergers <- mergePairs(dadaF = dadaForward,
                        derepF = forward,
                        dadaR = dadaReverse,
                        derepR = reverse,
                        verbose = TRUE)
  
  # construct sequence table of ASVs
  seqtab <- makeSequenceTable(mergers)
  
  # remove chimeras
  seqtab.nochim <- removeBimeraDenovo(seqtab, 
                                      method = "consensus",
                                      multithread = TRUE,
                                      verbose = TRUE)
  
  # print the percentage of chimeras
  cat("Frequency of non-chimeric sequences when accounted for abundance:", round(sum(seqtab.nochim)/sum(seqtab), 3))
  
  # assign taxonomy
  taxa <- assignTaxonomy(seqtab.nochim, DBPATH, multithread = TRUE)
  
  # add species assignment
  taxa <- addSpecies(taxa, SPECIESPATH)
  
  # final optional step: concatenate output to a phyloseq object
  if(PHYLO == TRUE) {
    
    # get sample names
    samples.out <- rownames(seqtab.nochim)
    sample.names <- sapply(strsplit(samples.out, PATTERNF), `[`, 1)
    # construct phyloseq object
    ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows = FALSE),
                   tax_table(taxa))
    
    # make phyloseq object more readable by changing DNA strings to ASV names
    dna <- Biostrings::DNAStringSet(taxa_names(ps))
    names(dna) <- taxa_names(ps)
    ps <- merge_phyloseq(ps, dna)
    taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
    
  }

  
} else { # perform the same workflow for single-paired reads
  
  # get reads
  forward <- sort(list.files(CLEANEDPATH, pattern = PATTERNF, full.names = TRUE))
  
  # perform error learning
  errF <- learnErrors(forward, multithread = TRUE)
  
  # perform dada2
  dadaForward <- dada(forward, err = errF, multithread = TRUE)
  
  # there is no merging!!
  
  # construct sequence table of ASVs
  seqtab <- makeSequenceTable(dadaForward)
  
  # remove chimeras
  seqtab.nochim <- removeBimeraDenovo(seqtab, 
                                      method = "consensus",
                                      multithread = TRUE,
                                      verbose = TRUE)
  # print the percentage of chimeras
  cat("Frequency of non-chimeric sequences when accounted for abundance:", round(sum(seqtab.nochim)/sum(seqtab), 3))
  
  # assign taxonomy
  taxa <- assignTaxonomy(seqtab.nochim, DBPATH, multithread = TRUE)
  
  # add species assignment
  taxa <- addSpecies(taxa, SPECIESPATH)
  
  # final optional step: concatenate output to a phyloseq object
  if(PHYLO == TRUE) {
    
    # get sample names
    samples.out <- rownames(seqtab.nochim)
    sample.names <- sapply(strsplit(samples.out, PATTERNF), `[`, 1)
    # construct phyloseq object
    ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows = FALSE),
                   tax_table(taxa))
    
    # make phyloseq object more readable by changing DNA strings to ASV names
    dna <- Biostrings::DNAStringSet(taxa_names(ps))
    names(dna) <- taxa_names(ps)
    ps <- merge_phyloseq(ps, dna)
    taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
    
  }
}

```

### References

dada2 tutorial: https://benjjneb.github.io/dada2/

Callahan et al 2016: https://www.nature.com/articles/nmeth.3869

Bioconductor dada2 manual: https://www.bioconductor.org/packages/release/bioc/manuals/dada2/man/dada2.pdf

SILVA databases: https://benjjneb.github.io/dada2/training.html (Silva version 132; training set for `assignTaxonomy` and species set for `addSpecies`)

dada2 original publication: https://f1000research.com/articles/5-1492

ASV vs OTU publication: https://www.nature.com/articles/ismej2017119

source code: https://github.com/benjjneb/dada2/tree/master/R



